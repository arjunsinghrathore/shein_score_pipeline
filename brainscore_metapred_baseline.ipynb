{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall tensorflow -y\n",
    "# !pip uninstall keras -y\n",
    "# !pip install tensorflow==2.6 --ignore-installed \n",
    "# !pip install keras==2.6\n",
    "\n",
    "# !pip install  efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from model_tools.activations.pytorch import load_preprocess_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_mean_imagenet = tf.constant([0.485, 0.456, 0.406], shape=[1, 1, 3], dtype=tf.float32)\n",
    "_std_imagenet =  tf.constant([0.229, 0.224, 0.225], shape=[1, 1, 3], dtype=tf.float32)\n",
    "\n",
    "def load_images(image_filepaths,image_size):\n",
    "    \n",
    "    return np.array([load_image(image_filepath,image_size) for image_filepath in image_filepaths])\n",
    "\n",
    "def load_image(image_filepath,image_size):\n",
    "    \n",
    "    original_image = cv2.imread(image_filepath)\n",
    "    height, width = original_image.shape[:2]\n",
    "    \n",
    "    if len(original_image.shape)==2:\n",
    "        original_image = gray2rgb(original_image)\n",
    "    #image = transform_gen.get_transform(original_image).apply_image(original_image)\n",
    "    \n",
    "    image = tf.image.resize(original_image,(image_size,image_size)).numpy()\n",
    "    image = tf.cast(image, tf.float32)/255.0\n",
    "    image -= _mean_imagenet\n",
    "    image /= _std_imagenet\n",
    "    \n",
    "    #inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "        \n",
    "    return image\n",
    "\n",
    "def load_preprocess_images(image_filepaths, image_size=256,**kwargs):\n",
    "    #torch.cuda.empty_cache()\n",
    "    images = load_images(image_filepaths,image_size)\n",
    "    return images\n",
    "\n",
    "preprocessing = functools.partial(load_preprocess_images, image_size=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_tools.activations.pytorch import PytorchWrapper\n",
    "from model_tools.activations.keras import KerasWrapper\n",
    "\n",
    "#activations_model = PytorchWrapper(identifier='my-model', model=MyModel(), preprocessing=preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "\n",
    "models_eff =glob.glob('/media/data_cifs/projects/prj_metapredictor/meta_models/models/eff*h5')\n",
    "models_saliency =glob.glob('/media/data_cifs/projects/prj_metapredictor/meta_models/models/saliency*h5')\n",
    "\n",
    "print('models_eff : ',models_eff)\n",
    "print('models_saliency : ',models_saliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models =[\n",
    "        # '/media/data_cifs/projects/prj_metapredictor/meta_models/models/vgg_baseline.h5',\n",
    "          '/media/data_cifs/projects/prj_metapredictor/meta_models/models/vgg_frosty_eon.h5',\n",
    "#  '/media/data_cifs/projects/prj_metapredictor/meta_models/models/resnet50_baseline.h5',\n",
    "         '/media/data_cifs/projects/prj_metapredictor/meta_models/models/saliency_volcanic_monkey.h5',\n",
    " '/media/data_cifs/projects/prj_metapredictor/meta_models/models/vgg_silver_moon.h5',\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_tools.activations.core import ActivationsExtractorHelper\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "class Tensorflow2Wrapper:\n",
    "    def __init__(self, identifier,model,preprocessing, *args, **kwargs):\n",
    "        import tensorflow as tf\n",
    "        self._model = model\n",
    "        \n",
    "        self._extractor = ActivationsExtractorHelper(identifier=identifier, get_activations=self.get_activations,\n",
    "                                                     preprocessing=preprocessing, *args, **kwargs)\n",
    "        self._extractor.insert_attrs(self)\n",
    "\n",
    "    @property\n",
    "    def identifier(self):\n",
    "        return self._extractor.identifier\n",
    "\n",
    "    @identifier.setter\n",
    "    def identifier(self, value):\n",
    "        self._extractor.identifier = value\n",
    "\n",
    "    def __call__(self, *args, **kwargs):  # cannot assign __call__ as attribute due to Python convention\n",
    "        return self._extractor(*args, **kwargs)\n",
    "    def batch_predict(self, inputs, batch_size=10):\n",
    "        activations = []\n",
    "        for batch_x in tf.data.Dataset.from_tensor_slices(inputs).batch(batch_size):\n",
    "            #print('here')\n",
    "            batch_act = self._model(batch_x)\n",
    "            batch_act = np.array(batch_act, np.float16)\n",
    "            activations += list(batch_act)\n",
    "        return np.array(activations, np.float16)\n",
    "    \n",
    "    def get_activations(self, images, layer_names):\n",
    "        layer_outputs = []\n",
    "        for layer in layer_names: \n",
    "            activation_model = tf.keras.Model(self._model.input, self._model.get_layer(layer).output)\n",
    "            #import pdb;pdb.set_trace()\n",
    "            layer_outputs.append(self.batch_predict(images))  # 0 to signal testing phase\n",
    "        return OrderedDict([(layer_name, layer_output) for layer_name, layer_output in zip(layer_names, layer_outputs)])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficientnet \n",
    "\n",
    "efficientnet.init_tfkeras_custom_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brainscore import score_model\n",
    "import pandas as pd\n",
    "from model_tools.brain_transformation import ModelCommitment\n",
    "from brainscore.benchmarks.public_benchmarks import SheinbergITPublicBenchmark\n",
    "from brainscore.utils import LazyLoad\n",
    "results = []\n",
    "for MODEL_NAME in models_saliency[4:]:\n",
    "    print('\\n##########################################################')\n",
    "    print('\\n##########################################################\\n')\n",
    "    print('\\n##########################################################\\n')\n",
    "    print(MODEL_NAME)\n",
    "    model = tf.keras.models.load_model(MODEL_NAME,compile=False)\n",
    "    layers = [n.name for n in model.layers[-4:]]\n",
    "    activations_model = Tensorflow2Wrapper(identifier=MODEL_NAME, model=model, preprocessing=preprocessing)\n",
    "    model = ModelCommitment(identifier=MODEL_NAME, activations_model=activations_model,layers = layers, region_benchmarks = {'IT': LazyLoad(SheinbergITPublicBenchmark)})\n",
    "    # score_v4 = score_model(model_identifier=MODEL_NAME, model=model,\n",
    "    #                 benchmark_identifier='dicarlo.MajajHong2015public.V4-pls')\n",
    "    # print(score_v4)\n",
    "    # score_it = score_model(model_identifier=MODEL_NAME, model=model,\n",
    "    #                 benchmark_identifier='dicarlo.MajajHong2015public.IT-pls',verbose=0)\n",
    "    # print(score_it)\n",
    "    # results.append([MODEL_NAME,score_it,score_v4])\n",
    "    # print(results)\n",
    "    # rdf = pd.DataFrame(results,columns=['model','score_it','score_v4'])\n",
    "    # rdf.to_csv('bs_score_our_models_eff.csv')\n",
    "\n",
    "    score_it = score_model(model_identifier=MODEL_NAME, model=model,\n",
    "                    benchmark_identifier='sheinberg.neural.IT-pls.1moreobf',verbose=0)\n",
    "    print('\\n##########################################################\\n')\n",
    "    print('\\n\\nscore_it : ', score_it)\n",
    "    \n",
    "    results.append([MODEL_NAME,score_it])\n",
    "    print('results : ',results)\n",
    "    rdf = pd.DataFrame(results,columns=['model','score_it'])\n",
    "    # rdf.to_csv(\"/media/data_cifs/projects/prj_brainscore/arjun_brainscore/bs_hackathon/metapred/shein_score_our_models_vgg_res_{}.csv\".format(MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rdf.iloc[0].keys() : ',rdf.iloc[0].keys())\n",
    "\n",
    "parse = []\n",
    "for i in range(len(rdf)):\n",
    "    row =  rdf.iloc[i]\n",
    "    model = row['model'].split('/')[-1]\n",
    "    raw_it_m = row['score_it'].raw.values[0]\n",
    "    raw_it_s = row['score_it'].raw.values[1]\n",
    "    # raw_v4_m = row['score_v4'].raw.values[0]\n",
    "    # raw_v4_s = row['score_v4'].raw.values[1]\n",
    "    ceiling_it_m = row['score_it'].ceiling.values[0]\n",
    "    ceiling_it_s = row['score_it'].ceiling.values[1]\n",
    "    # ceiling_v4_m = row['score_v4'].ceiling.values[0]\n",
    "    # ceiling_v4_s = row['score_v4'].ceiling.values[1]\n",
    "    parse.append([model,raw_it_m,raw_it_s,ceiling_it_m,ceiling_it_s])\n",
    "\n",
    "\n",
    "parsedf_eff = pd.DataFrame(parse,columns=['model','raw_it_m','raw_it_s','ceiling_it_m','ceiling_it_s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('parsedf_eff : ',parsedf_eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedf_eff.to_csv(\"/media/data_cifs/projects/prj_brainscore/arjun_brainscore/bs_hackathon/metapred/shein_score_our_models_vgg_res_{}.csv\".format(MODEL_NAME.split('/')[-1]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
